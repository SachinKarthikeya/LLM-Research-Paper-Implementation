I have implemented a research paper titled “A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges” from IEEE Xplore. The research paper explores the following key concepts:

- Introduction to Large Language Models
- Background of Large Language Models
- Hardware requirements for building Large Language Models
- Architectural overview of Large Language Models
- Different pre-trained Large Language Models developed and utilized in different automation tasks
- Domain-specific applications of Large Language Models

The research paper is primarily based on theoretical concepts of Large Language Models. My goal is to practically implement the pre-trained Large Language Models mentioned in the research paper to get a better understanding of how these Large Language Models work on different automation tasks for beginner-to-intermediate learners, including me. The pre-trained models that I have chosen for tasks are as follows:

1. TinyLLM - A model built from scratch for Text Generation
2. Phi-2 model by Microsoft for Question-Answering Conversation
3. BERT (Bidirectional Encoder Representations from Transformers) by Google for Text Summarization
4. Llama 3.2:1b by Meta for Conversational-based interaction

These implementations have greatly helped me understand the working of Large Language Models, and I hope they help others understand better about Large Language Models as well.

Original Research Paper from IEEE Xplore:- https://ieeexplore.ieee.org/document/10433480

“A Comprehensive Overview of Large Language Models” Research Paper from arXiv:-https://arxiv.org/pdf/2307.06435

“TinyLlama: An Open-Source Small Language Model” Research Paper from arXiv:- https://arxiv.org/pdf/2401.02385
